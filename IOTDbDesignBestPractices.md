# DolphinDB教程：物联网数据库设计实践

## 1.物联网数据库需求概述

在工业物联网场景中大部分的数据都有这样一些特征：都带有时间戳，且是按时间顺序生成的；大多为结构化数据；采集频率高，数据量大。物联网企业对时序数据库有以下几个方面的需求：
*  高速写入：由于传感器数量众多，而且每个传感器的采集频率都很高，所以写入的并发量会特别大，有时甚至会达到每秒上千万条数据写入。
*  降低存储成本：在存储容量有限的情况下，需要对数据做一定的压缩，以降低海量数据的存储成本。
*  设备的实时状态查询：要响应实时的查询请求，用于及时反映系统的状态。
*  快速的历史数据查询：数据的查询分析大多基于某一个时间段、某个设备或某些指标进行。由于历史数据量巨大，在查询时需要可以快速完成对特定时间段的数据的计算。
*  统计分析、机器学习和可视化的展示。
*  跨平台：工业物联网平台采用的设备通常种类众多，既有服务器或服务器集群，也有不同硬件架构的工控机和嵌入式设备。

## 2. DolphinDB数据库特点

* 支持对时间序列的友好建模，同时兼容宽列数据库、关系数据库的建模和查询功能

DolphinDB主要用于时序数据的存取和分析，对数据的频率或规律性没有要求，可以每纳秒、每毫秒或每小时收集一次，也可以以规则或不规则的间隔收集（例如，当某些事件发生时，而不是在预定的时间）。数据可以不按时间先后写入（即支持乱序写入），也可以重复写入（支持在查询时去重）。DolphinDB支持最高纳秒级精度的时间序列数据存储和处理，可以满足各类应用的需要。

DolphinDB兼容宽列数据库、关系数据库的功能，像传统的关系数据库一样易于使用。DolphinDB提供了标准协议的SQL查询接口，因此与现有的第三方分析可视化系统如Grafana、redash和帆软等可轻松实现集成与对接。而且，在OLAP领域已有的大量数据建模工作大都是基于关系模型展开的，DolphinDB支持关系模型，构建在传统关系数据库或数据仓库上的系统能直接沿用之前的经验成果，快速迁移到DolphinDB。

* 强大的分布式存储功能，可根据用户需要进行水平和垂直扩展

DolphinDB 内置了自研的分布式文件系统，以分区（Chunk）为单位对客户的海量数据进行管理。一个数据库或者一个数据库表通常包含多个分区，设备和时间是物联网领域最常用的分区维度。系统提供（VALUE）分区，范围（RANGE）分区，哈希（HASH）分区和列表（LIST）四种分区方式，用户可以灵活的进行组合使用，合理规划分区。

DolphinDB中的数据是列式压缩存储，它采用LZ4方式压缩数据，平均能达到20%-25%的无损压缩比。在查询时，加载数据的最小单位是分区列。DolphinDB不提供行级的索引，而是将分区作为数据库的物理索引。一个分区字段相当于给数据表建了一个物理索引。如果查询时用到了该字段做数据过滤，SQL引擎就能快速定位需要的数据块，而无需对整表进行扫描。物联网数据的查询分析大多基于某一个时间段、某个设备进行，因此分区字段选用时间和设备这两个维度，查询和计算的性能优异。

数据库的分区数据统一由分布式文件系统来存储和管理，一个数据库的数据可能分布存储在不同的服务器上，系统内部通过事务机制保证网络间数据的强一致性和完整性，对于外部用户来说，这些机制是完全透明的。基于分布式文件系统，用户可以方便对系统进行存储扩容。

* 融数据库，编程语言和分布式计算于一体。计算功能极其丰富，查询和计算速度快

DolphinDB计算功能丰富，支持常用的查询功能，包括范围查询，多维查询，聚合查询，对比查询，关联查询等。聚合查询不仅可以沿时间线和设备进行聚合，而且可以根据表中任意多个原生字段或计算字段进行聚合，也可以与关系数据库一样，关联后再进行聚合。DolphinDB内置了800多个函数，同时支持客户自定义函数在SQL中使用，支持通过多个SQL语句完成复杂的数据查询和清洗工作。DolphinDB不仅为用户提供了简单易用的SQL接口，而且提供了可定制开发的Map Reduce和Iterative Map Reduce等计算接口。基于此，客户可以实现更为复杂的数据分析和机器学习功能。

DolphinDB不仅计算功能丰富，而且提供了多种方法以实现海量数据的快速的查询和计算。这些方法包括：（1）分布式文件系统和多列组合分区方案能够支持单表千万级的分区，查询时能快速确定相关分区和所在节点，并且充分利用多节点多核的并行计算能力；（2）列式存储，只读取需要的的字段数据；（3）计算下推到存储引擎，避免了数据迁移所耗费的时间及资源；（4）自动缓存最新或最频繁使用的数据；（5）多种内存表辅助计算和缓存，避免中间结果落盘。

* 同时支持流计算和批量计算

DolphinDB提供了基于内存和磁盘的流数据表用于实时流计算处理，内置的聚合引擎可以按指定的时间窗口长度和频率来计算各种聚合指标。不同于传统的Lambda架构，即流计算用流计算平台，批计算用Map Reduce或者Spark，DolphinDB可以同时支持流计算和批量计算，满足物联网领域系统监控，预警，预测性维护等各种场景对实时流计算的各种需求。

* 支持高可用

DolphinDB允许为每一个分区保留多个副本，并通过改良的两阶段事务提交机制，确保数据写入时，同一副本在多节点之间的数据实现强一致性。

* 支持多种平台

DolphinDB是一个非常轻量级的系统，用GNU C++开发，核心程序仅20余兆字节，无任何依赖，可部署在边缘网关，本地平台或云端平台。支持32/64位Linux 和32/64位Windows操作系统。支持X86、ARM和MIPS指令体系。

## 3. 数据库设计

### 3.1 单值模型和多值模型介绍

数据库通用的设计模式有两种，即单值模式和多值模式：
*  单值模型针对测点建模。一个被监控的指标称为一个测点，例如“电量仪”中会有电压、电流、功率等测点，“温湿度计”中有温度、湿度测点。单值模型在写入时每一个测点为一行，所有采集的传感器指标值，无论种类，均存于同一列中。单值模型的数据集例子如下表所示，其中Timestamp列是时间，Value列是指标值，Tagname列是指标名，Description列是描述信息：

|Timestamp|Value|Tagname|Description|
|----|----|----|-----|
|2020.07.01T12:00:00.000|20.1|hz.1.speed|Speed/No.1 Wind Turbine in Hangzhou|
|2020.07.01T12:00:00.000|37.1|hz.1.temperature|Temperature/No.1 Wind Turbine in Hangzhou|
|2020.07.01T12:00:00.000|21.1|sh.2.speed|Speed/No.2 Wind Turbine in Shanghai|
|2020.07.01T12:00:00.000|38.1|sh.2.temperature|Temperature/No.2 Wind Turbine in Shanghai|

*  多值模型通常针对数据源（在物联网领域通常是设备）建模，每一行数据针对的是一个设备，一个设备在同一时刻被测量的所有指标值在同一行中。多值模型的数据集例子如下表所示，其中Timestamp是时间列，Speed和Temperature是指标列，No和Organization是维度信息：

|Timestamp|Speed|Temperature|No|Organization|
|----|----|----|-----|-----|
|2020.07.01 12:00:00.000|20.1|37.1|1|Hangzhou|
|2020.07.01 12:00:05.000|20.1|36.9|1|Hangzhou|
|2020.07.01 12:00:00.000|21.1|37.9|2|ShangHai|
|2020.07.01 12:00:05.000|21.0|37.9|2|ShangHai|

单值模型和多值模型在数据表示上可以进行相互转化，如多值模型可以用多条记录的单值模型来表示，同时多值模型也可以退化为只记录一项指标。因此，不管是单值模型还是多值模型都可以解决实际当中时序数据相关的问题。但是需要指出的是，两种模型在具体实现上还是有一些差异的，需要根据实际的业务场景选择合适的模型。

### 3.2 单值模型和多值模型比较和分析
#### 3.2.1 采集和写入

单值模型对每个测点在表中保存一行，因此不同指标的采集频率可以不同，比如有些指标可以200毫秒采集一次，有些可以1秒钟采集一次。此外，写入比较灵活，甚至可以采集到一个指标就写入数据库。多值模型要求同个设备的多个指标采集频率一致，写入时每个设备的多个指标是一行，因此要求这些指标的时间戳一致。

DolphinDB采用列式存储，一个分区内的一列数据单独存放在一个文件中。当写入数据时，若一次写入涉及很多文件，若对很多个物理文件进行操作（打开，写入，关闭等），就会严重影响性能。比如：
* 采用多值模型时，一般按时间和设备进行分区。如果每个设备有几千个指标，那么每进行一次数据写入，就要对几千个物理文件进行操作（打开，写入，关闭等），会严重影响数据库的写入性能。
* 采用单值模型时，一般按时间和测点进行分区。当单位时间内数据量小时，可以很多个指标写入同个分区。但当数据量非常大时，为了使每个分区数据的大小控制在合理范围，可能需要按每1个或几个指标进行分区，这时候分区数就会很多，那么每进行一次数据写入，也要操作很多个文件，也会严重影响数据库的写入性能。

DolphinDB提供了一种数据写入缓存机制--CacheEngine功能，能把少量多次的写入变成批次写入，减少磁盘的负载，提高写入性能。当多值模型指标数多、单值模型分区数多时，可启用CacheEngine功能。

#### 3.2.2  schema扩展性

在schema（表结构）扩展性方面，设备增加、减少指标时，单值模型一般不需要修改数据库的schema，甚至单值模型还能适应各设备指标数不一致的情况。而多值模型需要修改schema。比如在上节所示的数据集例子中，已有速度（speed）和温度（temperature）两个指标，若需再增加一个湿度（humidity）测量指标，单值模型只要为采集到湿度值增加行保存记录即可，但多值模型就要为表增加一个字段来存储采集到的湿度值。此外，扩展字段以后，很多涉及这个表的程序代码都需要相应修改。综上所述，多值模型在建表时可以预留字段为以后扩展。预留字段如果不用，在查询时不会加载到内存，不影响查询效率。

DolphinDB分布式表支持增加列，不支持修改、删除列。因此单值模型时，指标值Value这一列的数据类型设置需要慎重。譬如现在设置了整数类型，以后想增加指标值为浮点数的指标就不行，现在设置了float，以后就不能加值为double的指标。不管是单值模型还是多值模型，若列的数据类型需要修改，就得增加一个列或新建一个表，然后把旧的数据导入到新列或新表。

#### 3.2.3 存储空间

在数据冗余度方面，单值模型对每个指标都需要保存时间戳、指标标志（名称或编号）和指标值，而多值模型不需要保存指标标志、多个指标每行只有一个时间戳，以一个设备10个测量指标各采集一次数据为例，单值模型在表中保存10行，共保存30个值。而多值模型保存1个时间值+10个指标值，仅11个值。另外当各指标值有不同数据类型时，譬如有布尔值、整数和浮点数时，单值模型所有指标测量值都要按最大精度的数据类型浮点数存储，明显占用更多的存储空间。

在数据压缩方面，DolphinDB对每个分区内的数据采用列式增量压缩存储，采用的是LZ4压缩算法。从LZ4的压缩原理可知，若同一列中的重复项越多，压缩率就越高。多值模型同一个指标存成一列，在实际生产环境中，很多指标的测量值很少变，或随时间渐变，重复度较高，所以压缩效果较好。下面我们做个指标测量值都是随机值和都是固定值2个极端的实验，看一下压缩效果。假设有100个设备，每个设备采集1000个指标，所有数据类型都是float，每秒采集一次，按天和设备复合分区建立数据库，每10个设备为一个分区。然后以每批36000条记录写入一天的数据，测试结果如下：

|指标值 | 数据占用存储空间 | 压缩比  |
|----|-----|-----|
|随机值|33,970,680|12.6%|
|固定值|170,732|00.1%|

从上表可以验证，数值的重复度对压缩效果的影响很大。而单值模型若一个分区存储多个指标，各指标的测量值依次采集、交替存储，重复度一般比较低，压缩效果显然不如多值模型。

#### 3.2.4 数据查询

单值模型在数据库表中是一个测点保存一行记录，若一个设备有100个指标，所有指标各采集一次就要保存100行。而多值模型每个设备各指标采集一次却只需要保存一行。DolphinDB是列式存储，加载数据的最小单位是分区列。当用户查询分布式表时，只把用户所要求的分区和列加载到内存中。因此，同样的分区数，在查询某一个指标时，单值模型需要加载的数据通常是多值模型的许多倍。以一个设备100个指标保存一个分区为例，单值模型需要加载的记录数是多值模型的100倍。在数据量较大时，这会显著影响单值模型的查询效率，而且还可能出现内存不足导致加载失败的情况。

多值模型反映了数据的固有结构，保留了数据中的关系，可以很方便地在同一个时间序列下联合查询。以上节多值模型中数据集为例，要查2020.07.01 12:00:00.000-2020.07.01 12:00:05.000时间内的风风力情况，可以联合查询多个字段的值。而单值模型格式不够直观，如果不同指标之间要做很多协同处理，就不太合适。

为解决单值模型数据分析和可视化展示的障碍，DolphinDB提供了[pivot by](https://www.dolphindb.cn/cn/help/SQLStatements/pivotBy.html)功能。在查询的时候，pivot by功能可以把多个指标按时间对齐，返回一个二维表，其中每一行是时间，每一列是指标。pivot by功能应用示例如下：
```
tagId = `tagA`tagB`tagC`tagA`tagB`tagC`tagA`tagB`tagC$symbol; 				
value= 49.6 29.46 179.52 49.02 29.97 175.23 50.76 30.32 151.29;							
timestamp = [09:34:01.100,09:34:01.200,09:34:01.340,09:34:02.100,09:34:02.200,09:34:02.340,09:34:03.100,09:34:03.200,09:34:03.340];	
t = table(timestamp, tagId, value)
```
原始数据：

| timestamp    | tagId | value  |
|----|-----|-----|
| 09:34:01.100 | tagA  | 49.6   |
| 09:34:01.200 | tagB  | 29.46  |
| 09:34:01.340 | tagC  | 179.52 |
| 09:34:02.100 | tagA  | 49.02  |
| 09:34:02.200 | tagB  | 29.97  |
| 09:34:02.340 | tagC  | 175.23 |
| 09:34:03.100 | tagA  | 50.76  |
| 09:34:03.200 | tagB  | 30.32  |
| 09:34:03.340 | tagC  | 151.29 |

运行select value from t pivot by timestamp.second() as second, tagId  的结果：

|second   | tagA  | tagB  | tagC   |
|----|-----|-----|----|
| 09:34:01 | 49.6  | 29.46 | 179.52 |
| 09:34:02 | 49.02 | 29.97 | 175.23 |
| 09:34:03 | 50.76 | 30.32 | 151.29 |

使用pivot by功能后，单值模型的可视化展示与多值模型一样方便。

## 4.数据库设计模式应用案例
### 4.1 案例需求描述
某电厂有10台机组，每台机组的振动系统有3000个测点。具体需求如下：
* 对每个测点，每1秒钟采集一次数据。将采集到的原始数据存入数据库。
* 对测点按顺序进行编号：1--3000是一号机组，3001-6000是二号机组，其他以此类推。所有测点的数据类型都是float浮点数。
* 实时数据查询，查询各指标最新状态数据。
* 典型的历史数据查询是1个月，也可以多个月。做大数据分析时一般也是按照月进行查询和计算。
* 每次查询一个指标或多个指标，一次分析不会超过10个指标。

### 4.2 单值模型数据库设计方案
本案例若按单值模型设计，表结构可定义如下，其中1--3000是一号机组，3001~6000是二号机组，其他以此类推：

|列名|数据类型|备注|
|----|-----|-----|
|id	 | INT|指标编号|
|tm	 | DATETIME|时间|
|val | FLOAT|指标值|

每一秒采集30000个指标一次，每天新增的数据约30GB，每月约900GB。这样的数据量级，传统的关系型数据库已难以应对，但使用DolphinDB的分区机制，却可以轻松应对TB甚至PB级别的数据量。常用的查询基于时间段和指标，因此分区按时间和指标这2个维度进行就比较合理。时间维度一般情况下按天进行值（VALUE）分区，但也要根据数据量和典型查询场景具体情况具体分析。若某些场景下，查询时间跨度不是很长，而单位时间内的数据量又非常大，也可以按小时进行分区。若数据量比较小，又常常按一个月或几个月进行查询和计算，也可以按月分区。在本案例中，常见的查询是按月查询一个或多个指标，那就可以考虑按月分区。

物联网设备监控场景，各指标的采集频率一般固定，比较容易计算单位时间的数据量大小。因此指标维度的分区通常采用范围（RANGE）或值（VALUE）方法。DolphinDB中组合分区的多个列在逻辑上是并列的，不存在从属关系或优先级关系。如果时间维度有n个分区，指标维度有m个分区，最多可能有n \* m个分区。DolphinDB在解决海量数据的存取时，并不提供行级的索引，而是将分区作为数据库的物理索引。为提高查询和计算性能，每个分区的数据量不宜过大、也不宜过小。一般建议每个分区压缩前的数据量控制在100MB左右（详细分析可参阅[分区数据库教程](./database.md)）。

本案例的查询和计算主要按月和指标进行，所以数据库按照两个维度分区，第一个按月分区，第二个按照指标分区设计。每个指标每月的数据约30MB（86400条/天\*30天\*12字节/条）。所以第一个分区维度采用值分区，设置每月为一个分区，第二个分区维度采用范围（RANGE）分区，设置每3个指标为一个分区,每个分区的大小约90MB。参考代码如下：
```
def createDatabase(dbName,tableName){
	tableSchema = table(100:0,`id`tm`val,[INT,DATETIME,FLOAT]);
	db1 = database("",VALUE,2017.01M..2020.12M)
	db2 = database("",RANGE,0..10000*3+1)
	db = database(dbName,COMPO,[db1,db2])
	dfsTable = db.createPartitionedTable(tableSchema,tableName,`tm`id)
}
```
### 4.3 多值模型数据库设计方案
本案例若用多值建模，表结构定义如下：

|列名|数据类型|备注|
|----|-----|-----|
|id	 | INT|机组编号|
|tm	 | DATETIME|时间|
|TAG0001| FLOAT|指标值1|
|... | ||
|TAG3000 | FLOAT|指标值3000|

多值建模时，物联网监控场景常用的查询基于时间段和设备，因此分区字段宜选择时间和设备这2个维度。时间维度通常按天、按小时或按月进行值（VALUE）分区，具体选哪个需要根据数据量和典型查询场景进行具体分析和设计。若查询时间段跨度不大、单位时间内采集数据量大，可按小时进行分区，若查询时间段跨度大、单位时间内数据量不大，可按月分区。设备维度的分区可以采用哈希、范围、值、列表等多种方法，原则就是要根据业务特点对数据进行均匀分割，让每个分区压缩前的大小控制在100MB左右。但如果数据表为宽表（几百甚至几千个字段），若单个应用只会使用一小部分字段，因为未用到的字段不会加载到内存，不影响查询效率，这种情况可以适当放大分区上限的范围。另外，在实际应用中，数据库分区也可按三个维度进行。比如我们采集设备的网络日志，常用的查询基于时间段、源IP地址和目的IP地址，我们可以考虑采用时间、源IP地址和目的IP地址三个分区维度。

本案例每机组每天的记录数为86400条，每天的数据量约1GB。常用查询按月和设备进行，因此数据库分区最好按照两个分区维度设计，第一个按月分区，第二个按照机组分区。每月每设备的数据量约30GB，似乎不适用按月分区。但我们注意到查询时每次最多10个指标，也就是说查询时最多加载10个分区列。而每机组每10个指标每月数据约118MB（86400条/天\*30天\*48字节/条），所以我们还是可以按月值分区和按设备值分区。本案列的参考代码如下：
```
def createDatabase(dbName,tableName){
	m = "tag" + string(decimalFormat(1..3000,'0000'))
	tableSchema = table(100:0,`devID`ts join m ,[INT,DATETIME] join take(FLOAT,3000) )
	db1 = database("",VALUE,2017.01M..2020.12M)
	db2 = database("",VALUE,1..10)
	db = database(dbName,COMPO,[db1,db2])
	dfsTable = db.createPartitionedTable(tableSchema,tableName,`ts`devID)
}
```
需要注意的是，上述2个方案中指标和机组都只存储了编号，若有指标和机组的描述信息，在查询分析时又需按这些信息进行关联查询和计算，可以建立维度表。维度表是分布式数据库中没有分区的表，一般用于存储不频繁更新的小数据集。维度表不分区，但是数据的写入和分区表无异。维度表可与任何分布式表，维度表以及内存表进行关联。DolphinDB自动会在各个节点上缓存(cache)维度表，以提升查询和关联的性能。
### 4.4 两种设计方案性能对比测试
#### 4.4.1 写入性能对比测试
本案例单值模型数据库设计方案的指标分区有10000个，每分区有3列。多值模型设计方案的设备分区有10个，每分区有3001列。每个指标每写入一次数据都涉及3万个文件，对这么多文件进行操作（打开，写入，关闭等）非常耗时，因此启用了Cache Engine功能。Cache Engine是DolphinDB中的一种数据写入缓存机制，能把少量多次的写入变成批次写入，减少磁盘的负载，提高写入性能（详情可参考[Cache Engine与数据库日志](./redoLog_cacheEngine.md)教程）。
测试服务器配置如下：

|部件|配置|
|----|-----|
|主机|PowerEdge R730xd|
|CPU|E5-2650，24 核 48线程|
|内存|512GB|
|硬盘|HDD  1.8T x 12|
|OS|CentOS Linux release 7.6.1810|

DolphinDB服务器为single mode模式。配置如下：
```
maxMemSize=32	//32G内存限制
workerNum=16	//worker个数
dataSync=1	//启用Redo Log模式，保证数据不丢失
chunkCacheEngineMemSize=8	//启用Cache Engine，缓存数据，提升系统吞吐量
```

两种建模设计方案的测试在同等条件下进行，结果如下：

|写入批次大小（行）|单模方案写入吞吐量（百万数据点/秒）|多模方案写入吞吐量（百万数据点/秒）
|--|--|--|
|多值600/单值600*3000|    6.3|	22.8|
|多值36000/单值36000*3000|	50.4	|136.2|

从上表可以看出，每批写入记录数多可大大提升写入吞吐量，在每批同样多测点数写入时，多值模型的写入性能约是单值模型的3倍。这个主要是因为单值模型的数据冗余度比较大的缘故，多值模型写入一行数据3001\*4个字节，单值模型要写入3000\*12个字节。

#### 4.4.2 历史数据查询性能对比测试
在数据库存有3个月历史数据的情况下，分别测试查询1个指标和10个指标一个月数据的耗时：
```
svDfsTable=loadTable("dfs://singleval","tn8k")
timer select tm,id,val from svDfsTable where id =1, tm  between 2020.01.01T00:00:00 : 2020.01.31T23:59:59   //1551ms
timer select tm,id,val from svDfsTable where id  in (1..10), tm <= 2019.01.31, tm >= 2019.01.01 //6338ms

mvDfsTable=loadTable("dfs://multival","tn8k")
timer select ts,tag2001 from mvDfsTable where devID=1,  ts  between 2020.01.01T00:00:00 : 2020.01.31T23:59:59   //633ms
timer select ts,tag0001,tag0002,tag0003,tag0004,tag0005,tag0006,tag0007,tag0008,tag0009,tag0010 from mvDfsTable 
where devID=1, ts  between 2020.01.01T00:00:00 : 2020.01.31T23:59:59  //2549ms

```
再查询一周时间内的10个测点的趋势数据，按每2分钟分成一段，每个时间段里都计算最大值，最小值，开始值，结束值：
```
timer select first(val) as firstw,last(val) as lastw,max(val) as maxw,min(val)  as minw from svDfsTable 
where id in(0..9)*3000+1 ,tm between 2020.01.01T00:00:00 : 2020.01.07T23:59:59  group by id,bar(tm,120) ; //15546ms

timer select first(tag0001) as firstw,last(tag0001) as lastw,max(tag0001) as maxw,min(tag0001)  as minw from mvDfsTable 
where ts between 2020.01.01T00:00:00 : 2020.01.07T23:59:59 group by devID,bar(ts,120) ; //3720ms
```

如上所示，查询单个指标一个月时，单值模型和多值模型的耗时是1551毫秒VS633毫秒，10个指标时是15546毫秒VS3720毫秒。查询趋势数据的耗时是15546毫秒VS3720毫秒。显而易见，多值模型的查询、计算性能占优，主要是因为查询时，多值模型只要加载相关分区的时间列和相应指标列，而单值模型要加载相关分区的时间列、指标编号列和值列，加载的数据要相应多一些。
#### 4.4.3 对比测试小结
无论是写入性能还是查询性能，多值模型数据库设计方案都比单值模型优越。而且多值模型数据库的存储空间也比单值模型小得多，因此设计时优选多值模型。但如前面第三节所述，多值模型相比单值模型的缺点就是要求各指标写入时间对齐、schema扩展性差。因此在设计时，需要根据具体情况具体分析，综合考虑。
### 5. 实时状态查询
#### 5.1 快照引擎的应用
DolphinDB提供了快照引擎（Snapshot Engine）以快速获取任意设备任意指标的最新状态。物联网应用中，对设备的最新状态进行监控是一个刚性需求。尽管数据库会对最新或最频繁使用的数据进行缓存，但这不是百分之百保证的。快照引擎则会把所有设备采集的实时数据更新到哈希表，查询时可以快速返回结果。一旦客户为数据表设置了快照引擎，写入和查询对用户都是透明的，无需额外的工作即可以非常小的内存代价获取优异的响应速度。

快照引擎保存的是最新插入记录的快照，若系统插入的数据是乱序的，即系统不能保证按时间顺序插入数据，则不适合使用快照引擎，否则得到的不是最近时间的数据。因为保存了每个分组的快照，当分组很多时，会占用很多内存，若系统内存资源紧张，也不建议使用快照引擎。在数据节点启动后，需要注册快照引擎后，节点才会保存最新记录的快照。因此，若插入记录的时间间隔比较长，在重启后到新的数据写入之前，这段时间数据节点是没有保存快照的，所以这时若需要查询快照注册之前的最新记录，是不能用快照引擎查询到最新记录的。

在DolphinDB中，除了快照引擎，还有三种通过select语句在线计算的方法获取最新记录，当快照引擎不适用时，可以考虑用这些方法替代：
* 通过group by对表内记录分组，然后用[last](https://www.dolphindb.cn/cn/help/FunctionsandCommands/FunctionReferences/l/last.html)函数查找每个分组的最后一行记录。但它也与快照引擎一样，不适合插入的数据是乱序的情况。
* 通过group by对表内记录分组，然后用[atImax](https://www.dolphindb.cn/cn/help/FunctionsandCommands/FunctionReferences/a/atImax.html)函数找出每个分组的时间戳这列的最大值（即最新时间）所在行，然后返回该行的其他列数据。这种方法可用于插入数据是乱序的情况。
* 通过SQL扩展语句[context by](https://www.dolphindb.cn/cn/help/SQLStatements/contextBy.html)对表内记录进行分组，然后在组内按时间戳列进行csort降序排序，再和top一起使用，就可以用于获取每个分组中的最新记录。这种方法也可用于插入数据是乱序的情况。
#### 5.2 快照引擎的性能
首先，准备测试环境。部署三台物理服务器的6数据节点的DolphinDB集群后，我们在DolphinDB GUI中用下列代码创建分布式数据库testDB和分布式表trainInfoTable，表中除了ts时间字段、trainID分组字段，还有3个指标字段。在 DolphinDB database中的分区方案是将时间ts作为第一个维度，每天一个分区；再将trainID作为分区的第二个维度，共分30个区。
```
def createDatabase(dbName){
	tableSchema = table(100:0,`trainID`ts`tag0001`tag0002`tag0003 ,[INT,TIMESTAMP, FLOAT,DOUBLE,LONG])
	db1 = database("",VALUE,today()..(today()+3))
	db2 = database("",RANGE,0..30*10+1)
	db = database(dbName,COMPO,[db1,db2])
	dfsTable = db.createPartitionedTable(tableSchema,"trainInfoTable",`ts`trainID)
}
createDatabase("dfs://testDB")
```
用下列代码实时写入数据到分布式表中，300个设备，每200毫秒为每设备写入一次，其中插入的数据都是有序的。
```
//为每设备产生一条记录
def simulateData(trainVector){
	num = size(trainVector)
	return table(take(trainVector,num) as trainID ,take(now(),num) as ts,rand(20..41,num) as tag0001,rand(30..71,num) as tag0002,rand(70..151,num) as tag0003)
}
//为分组中的每个设备写入一批记录，重复batches次
def simulate(host,port,trainVector,batches){
	h = xdb(host,port,"admin","123456")
	for(i in 0:batches){
		t=simulateData(trainVector)
		h("append!{loadTable('dfs://testDB', 'trainInfoTable')}", t)
		sleep(200)
    }
}
```
然后注册快照引擎并查询，代码如下：
```
registerSnapshotEngine("dfs://testDB", "trainInfoTable", `trainID)
timer select [HINT_SNAPSHOT] * from loadTable("dfs://testDB", "trainInfoTable")
```
通过group by对表内记录分组，然后用[last](https://www.dolphindb.cn/cn/help/FunctionsandCommands/FunctionReferences/l/last.html)函数查找每个分组的最后一行记录。为了减少查询范围，增加了一个查询条件，即限制ts为最近一小时。代码如下：
```
timer select  last(ts),last(tag0001),last(tag0002),last(tag0003)
from loadTable('dfs://testDB', 'trainInfoTable') where ts >datetimeAdd(now(),-1,`h)  group by trainID
```
通过group by对表内记录分组，然后用[atImax](https://www.dolphindb.cn/cn/help/FunctionsandCommands/FunctionReferences/a/atImax.html)函数找出每个分组的时间戳这列的最大值（即最新时间）所在行，然后返回该行的其他列数据即表的最新记录。为了减少查询范围，增加了一个查询条件，即限制ts为最近一小时。查询代码如下：
```
timer select atImax(ts,tag0001 ) as tag0001,atImax(ts,tag0002 ) as tag0002,atImax(ts,tag0003 ) as tag0003
from loadTable('dfs://testDB', 'trainInfoTable') where ts >datetimeAdd(now(),-1,`h)  group by trainID
```
通过SQL扩展语句[context by](https://www.dolphindb.cn/cn/help/SQLStatements/contextBy.html)对表内记录进行分组，然后在组内按时间戳列进行csort降序排序，再和top一起使用，就可以用于获取每个分组中的最新记录。为了减少查询范围，增加了一个查询条件，即限制ts为最近一小时。查询代码如下：
```
timer select top 1 * from loadTable('dfs://testDB', 'trainInfoTable') where ts >datetimeAdd(now(),-1,`h)   context by trainID csort  ts desc 
```
在测试时，我们按数据量分2种情况测试，一是刚开始往数据库中写入数据时，测试时当天的分区中的数据记录大约有10万条。二是长时间写入后，测试时当天的分区中的数据记录已有约4千万条。测试结果如下表：

|方法	         |第一次测试（刚开始写入时，单位：毫秒）|第二次测试（长时间写入后，单位：毫秒）|
|------------    |---|---|
|snapShotEngine  | 53| 55 |
|last            | 70| 723|
|atImax          | 71| 743|
|context by      | 72| 781|

从测试结果可以发现，用快照引擎查询最新记录的性能是最优的。而且由于它是返回内存中的快照，不受分区中记录数的影响，每次查询的时间都变化不大。而另外三种方法在查询时，会先把分区中的数据加载到内存中，所以当分区中的数据比较多时，会严重影响查询性能。

### 6. 实时计算
#### 6.1 聚合引擎介绍
物联网中常常有实时计算的需求，例如计算每个设备过去1分钟的平均温度指标，计算的频率为每2秒钟一次。这时候就可以把采集的数据写入DolphinDB的实时流数据表，实时流数据表既可用于实时数据的快速查询，也可用于计算和监控。DolphinDB数据库的流计算目前已支持时序聚合引擎，横截面聚合引擎，异常检测引擎和自定义流计算引擎:
* 时序聚合引擎(Time-Series Aggregator)

能对设备状态进行纵向聚合计算(按时间序列聚合)，或者将多个设备状态横向聚合后再按时间聚合。时序聚合支持滑动窗口的流式计算。DolphinDB对内置的窗口聚合函数均进行了性能优化，单核CPU每秒可完成近百万状态的时序聚合。

* 横截面聚合引擎(Cross Sectional Aggregator)

是快照引擎的扩展，能对设备状态进行横向聚合计算，比如计算一批设备的温度均值。
* 异常检测引擎(Anomaly Detection Engine)

能实时检测数据是否符合用户自定义的警报指标，如发现异常数据，将它们输出到表中，满足物联网实时监控和预警的需求。

当以上三种引擎都不能满足需求时，用户也可以使用DolphinDB脚本或API语言自定义消息处理函数。当新的数据到来时，系统通知客户端应用进行处理。处理的结果可以写入数据库，写入内存表，写入外部的消息系统譬如OPC、MQTT等。处理的结果也可以作为中间结果写入另一个流表，实现链式的流计算处理。目前支持流数据的API语言包括C++, Java, C#，Go和Python。

#### 6.2 聚合引擎应用案例
##### 6.2.1 案例需求描述
 某企业用户需要建立一个数控机床的监控平台，便于随时了解自己生产线的工作情况，提高生产效率和智能化，也能及时获得异常告警，从而合理安排生产，规避风险等。
* 机床数据采集频率是一秒钟一次。
* 实时计算每个机床过去一分钟的主轴转速指标的最大值和平均值，计算的频率为每两秒钟一次。
* 实时统计各状态机床的总数，即处于加工中、故障、空闲的状态的机床各有多少台，计算的频率为每一秒钟一次。
* 当机床状态为故障时报警。
##### 6.2.1 案例设计
流数据表作为发布和订阅流数据的载体，发布一条消息等价于往流数据表插入一条记录。因此我们首先定义一个cncSt流数据表用于接收实时采集的机床数据，并用[enableTableShareAndPersistence](https://www.dolphindb.cn/cn/help/FunctionsandCommands/CommandsReferences/e/enableTableShareAndPersistence.html)函数对cncSt表做持久化，内存中保留的最大数据量是100万行。虽然机床设备有很多指标，因为本例只涉及2个，所以我们对表结构进行了简化。
```
st=streamTable(1000000:0,`deviceID`ts`RunningStatus`spindleSpeed,[INT,TIMESTAMP,CHAR,INT])
enableTableShareAndPersistence(st,`cncSt,false, true, 1000000)
```
用[createTimeSeriesAggregator](https://www.dolphindb.cn/cn/help/FunctionsandCommands/FunctionReferences/c/createTimeSeriesAggregator.html)函数创建一个时序聚合引擎， 实时计算每个机床过去一分钟的主轴转速指标的平均值和最大值。函数第二个参数指定了窗口大小为60秒，第三个参数指定每2秒钟做一次求均值运算，第四个参数是运算的元代码，可以由用户自己指定计算函数，任何系统支持的或用户自定义的聚合函数这里都能支持，本例中用avg计算平均值、max计算最大值。通过指定分组字段deviceID，函数会将流数据按机床分别进行运算，每个机床都会按各自的窗口计算得到对应的统计值。然后通过[subscribeTable](https://www.dolphindb.cn/cn/help/FunctionsandCommands/FunctionReferences/s/subscribeTable.html)订阅流数据，在有新数据进来时触发实时计算，并将运算结果保存到一个新的数据流表cncCompSt中。
 ```
share streamTable(1000000:0, `time`deviceID`spindleSpeedAvg`spindleSpeedMax, [TIMESTAMP,INT,INT,INT]) as cncCompSt
metrics = createTimeSeriesAggregator(name="tsAggr1",windowSize=60000,step=2000,metrics=<[avg(spindleSpeed),max(spindleSpeed)]>,dummyTable=cncSt,outputTable=cncCompSt,timeColumn=`ts,keyColumn=`deviceID,garbageSize=1000) 
subscribeTable(, "cncSt", "metricEngine", -1, append!{metrics},true)
```
用[createCrossSectionalAggregator](https://www.dolphindb.cn/cn/help/FunctionsandCommands/FunctionReferences/c/createCrossSectionalAggregator.html )函数创建一个横截面聚合引擎，实时计算所有机床的指标统计值。函数第二个参数是运算的元代码，它可以是系统内置或用户自定义的函数，如<[sum(col1), avg(col2)]>，可以对聚合结果使用表达式，如<[avg(col1)-avg(col2)]>，也可以对计算列进行聚合运算，如<[std(col1-col2)]>。第五个参数指定字段deviceID为输入设备的key。第六个参数是表示触发计算的方式。有三个可选项："perRow"，每插入一行数据触发一次计算；"perBatch"，每插入一次数据触发一次计算；"interval"，按一定的时间间隔触发计算。本例取"perBatch"表示插入一批数据计算一次。通过subscribeTable订阅流数据，在有新数据进来时触发实时计算，并将运算结果保存到一个新的数据流表allCncSt中。
```
allCncSt = table(1:0, `time`count0`count1`count2, [TIMESTAMP,INT,INT,INT])
crossAggregator1=createCrossSectionalAggregator(name="crossSectionalAggr",metrics= <[sum(RunningStatus==0), sum(RunningStatus==1),sum(RunningStatus==2)]>, dummyTable=cncSt, outputTable=allCncSt, keyColumn=`deviceID, triggeringPattern=`perBatch)
subscribeTable(,"cncSt","crossAggregator1",-1,append!{crossAggregator1},true)
```
用[createAnomalyDetectionEngine](https://www.dolphindb.cn/cn/help/FunctionsandCommands/FunctionReferences/c/createAnomalyDetectionEngine.html)函数创建一个异常检测引擎，当机床的状态为故障时报警。
```
share streamTable(1000:0, `deviceID`ts`anomalyType`anomalyString, [INT, TIMESTAMP, INT, SYMBOL]) as outputTable
engine1 = createAnomalyDetectionEngine("engine11", <[RunningStatus == 0]>, cncSt, outputTable, `ts, `deviceID, 1000, 1)
subscribeTable(, "cncSt", "sensorAnomalyDetection", 0, append!{engine1}, true)
```
### 7. 数据维护和管理

DolphinDB数据库的分区统一由内置的分布式文件系统来存储和管理。用户既可以按照自己的需要，用脚本手动删除指定的分区，也可以为每个数据库分别设置数据保留策略。数据库一旦设置保留策略，过期数据会自动删除。

DolphinDB支持通过脚本实现不同数据库间的复制与同步，数据库的备份（增量或全量）与恢复，数据在各种不同的存储设备之间的迁移。与数据删除一样，上述操作同样以分区为单位。如果是经常性的操作，这些命令（脚本）可以形成一个作业，并设定以固定时间和频率在数据库中运行。命令（脚本）和定时作业（scheduled job）的结合，可让用户以最大的灵活性快速实现精细化、自动化的数据维护和管理。

#### 7.1 临时的数据删除
在分析场景中，数据的价值随着时间流逝而不断降低，多数业务出于成本考虑只会保留最近几个月或最近几年的数据。DolphinDB主要是为海量结构化数据存储、检索、分析和计算设计的，目前不支持交易型场景需要的单条记录修改和删除功能，但支持以分区为单位进行批量删除和修改。

DolphinDB提供了[dropPartition](https://www.dolphindb.cn/cn/help/FunctionsandCommands/CommandsReferences/d/dropPartition.html)函数删除分区数据。通过dropPartition函数，可以删除指定的分区，也可以删除符合指定条件的分区。在删除之前，可以先用[schema](https://www.dolphindb.cn/cn/help/FunctionsandCommands/FunctionReferences/s/schema.html)获取分区信息。比如对数据库运行schema如下：
```
>schema(database("dfs://db1"))
partitionSchema->([2020.05M,2020.04M,2020.03M，2020.02M,2020.01M，2019.12M,2019.11M,2019.10M,2019.09M,2019.08M,],[1,11,21,31,41,51,61,71,81,91,...])
databaseDir->dfs://db1
partitionSites->
partitionType->[1,2]
```
可以知道它是复合分区，第一个维度按月值分区，第二个维度是范围分区。现在删除2019年8月的过期数据、删除2019.09M/1_11分区数据可以用下面代码：
```
dropPartition(database("dfs://db1"),2019.08M);
dropPartition(database("dfs://db1"),[2019.09M,1]);
```
#### 7.2 设置数据删除规则
对分区方案包含DATE类型或DATEHOUR类型的分布式数据库，DolphinDB也支持通过数据保留策略(Retention Policy)自动删除过期数据。
保留策略通过[setRetentionPolicy](https://www.dolphindb.cn/cn/help/FunctionsandCommands/CommandsReferences/s/setRetentionPolicy.html)设置，通过schema查看。如下例所示，我们对数据库设置了7天的数据保留时间：
```
>db=database("dfs://db1",VALUE,2019.06.01..date(now()))
retentionHour=7*24
setRetentionPolicy(db,retentionHour,0);

>schema(db);
partitionSchema->[2019.07.03,2019.07.02,2019.07.01,2019.06.30,2019.06.29,2019.06.28,2019.06.27,2019.06.26,2019.06.25,2019.06.24,...]
databaseDir->dfs://db1
partitionSites->
retentionDimension->0
retentionHours->168
```

### 8. 数据库的可靠性设计
#### 8.1 冗余和负载均衡
主流的分布式数据库大多采用主从架构，主节点不仅要负责管理元数据和状态同步，还要双机热备来保证高可用，容易成为系统瓶颈，增加系统横向扩展的难度。DolphinDB采用对等架构，依靠全局可见的元数据服务，任何数据节点都可以成为查询请求和数据写入的入口，不存在系统瓶颈点，更容易做到资源的负载均衡。

DolphinDB允许为每一个分区保留多个副本，并通过改良的两阶段事务提交机制，确保数据写入时，同一副本在多节点之间的数据实现强一致性。设置冗余数据的好处有两个：
* 当某个数据节点失效或者或磁盘数据损坏时，系统的容错功能保证 继续提供服务； 
* 当大量并发用户访问时，多副本提供负载均衡的功能，提高系统吞吐量，降低访问延时。这一点相比采用Paxos、Raft等一致性算法的系统有明显优势。这些系统虽然有多个副本，但同时只有一个副本可用（即只有Leader提供服务）。
#### 8.2 考虑停电的影响
一般操作系统为了提升I/O性能，都会提供页面缓冲。当数据写入文件时，并没有直接写入到磁盘，而是写到操作系统的缓冲页面中。如果操作系统崩溃或掉电，可能会数据丢失。若要保证数据不丢失，就需要每次把写入的数据刷盘（fsync），但硬盘fsync比较耗时，严重影响数据库写入性能。为了解决断电、数据库系统宕机等极端情况下的数据一致性问题，又尽量不影响数据库的实时写入性能，DolphinDB引入了Redo Log (数据库日志)功能。

Redo Log在每次写入时，先把描述更改的日志记录到日志文件，对日志文件刷盘后，再采用异步写入的方式写入数据文件。这大大减少了磁盘的写入次数。而且Redo Log采用了顺序写入，相对随机写入，性能更好。DolphinDB中一张表的每一列都分别存储为一个文件，当列数特别多时，这种性能差异尤为明显。因为Redo Log额外写了Redo Log文件，系统内部对还未写到磁盘上的数据文件进行了缓存，因此开启Redo Log后，系统的整体负载会有上升，写入性能也会下降（下降幅度与实际的数据有关，一般在20%左右）。另外因为启动时需要对上次遗留的Redo Log进行重做，重做过程当中分布式数据库处于不可用状态，所以系统的启动时间也有可能增加。Redo Log主要用于对数据可靠性要求比较高的实时写入的场景。如果对实时写入的数据可靠性要求不高，或只是用于历史数据分析，可以考虑不开启Redo Log。

### 9. 数据库性能和成本
#### 9.1 存储设备的选用
DolphinDB对数据库的写入提供了两种策略：一种是在每次写入后强制刷入磁盘；另一种是每次写入后不强制刷盘，由操作系统决定什么时候更新到磁盘。强制刷盘策略保证了数据的安全，但是会降低数据库的写入性能。非强制刷盘策略提高了写入性能，但操作系统崩溃或掉电，可能会导致数据丢失。因此在生产环境中，我们建议配置两种不同的硬盘，将元数据、Redo Log等数据库日志写入容量较小但性能较高的固态硬盘或者高性能IO设备，将查询日志、系统运行日志、后台任务日志等监控类日志和海量数据写入容量大但性能较低的机械硬盘，这样既经济又能提升数据库性能。
#### 9.2 性能及机器资源配置

在集群部署方面，合理的参数配置，使系统均衡合理的使用这些资源，能充分发挥机器的性能：
* cpu：配置workerNum和localExecutors，workerNum和localExecutors直接决定系统并发处理能力，应多个数据节点平均分配，尽可能利用主机所有的cpu并发处理能力。
* 内存：配置节点使用的最大内存量maxMemSize。内存对于改进计算节点的性能非常明显，尽可能高配。但要根据物理内存的大小来调整，否则操作系统内部内存不足时，会杀死进程。
* 磁盘：配置两种不同的硬盘，将元数据、RedoLog等数据库日志写入容量较小但性能较高的固态硬盘或者高性能IO设备；将海量数据写入容量较大的机械硬盘。不同的数据节点配置不同的磁盘，每个数据节点挂载多卷（volumes），以并行利用多个磁盘的I/O接口。若磁盘卷是RAID5磁盘阵列，建议设置diskIOConcurrencyLevel=4， 提高并行读数据的能力。

以上述第4节多值模型的测试环境为例，增加下列配置：
```
diskIOConcurrencyLevel=4
volumes=/hdd/hdd7/qxjtestiot,/hdd/hdd8/qxjtestiot,/hdd/hdd9/qxjtestiot,/hdd/hdd6/qxjtestiot
```
然后把历史数据重新写入多值模型数据库后，可看到2020年01月的各个设备的分区数据分布在4个硬盘上，再按上述语句查询一周的趋势数据，耗时能减少2-3倍。

* 网络：服务器之间用万兆网络连接，提高网络性能。

#### 9.3 提升写入和查询性能的措施

提升写入性能的措施包括：
* 采用多个客户端多线程并行写入，但要注意多个writer不能同时往同一个分区写入。
* 启用RedoLog功能和写入数据缓存(CacheEngine)功能，这个采用同步写入确保数据安全，也就是说只有元数据和RedoLog刷入磁盘后，才返回结果给客户端。如用于研究等非生产环境，也可采用异步写入，写入的延迟和吞吐量可以进一步改进。
* 批量写入，每次写入数据量不宜过小，一般每次数据量写入在几十兆字节比较合适。
* 每次写入数据涉及的分区数据量不宜过多。DolphinDB按照分区来列式存储，比如每次写入1000分区10列，那么每次需要写10000(1000*10)个文件，会严重影响写入性能。

提升查询性能的措施包括：
* 合理的分区设计非常重要。在物联网领域，一般用时间和设备两个维度来进行分区。时间维度可按天、按月等进行分区，具体选哪个需要根据数据量和典型查询场景进行具体分析和设计。设备标识维度的分区可以采用哈希、范围、值、列表等多种方法，原则就是要根据业务特点对数据进行均匀分割。DolphinDB不提供行级的索引，而是将分区作为数据库的物理索引。因此每个分区的数据量不宜过大。一般我们建议一个表在一个分区的数据量在100MB左右。


